{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "id": "EgOaniCiayes",
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:10.798429Z",
     "start_time": "2024-08-09T08:22:10.783420Z"
    }
   },
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:11.956059Z",
     "start_time": "2024-08-09T08:22:11.047727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# Create the language model\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.0)\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\n",
    "    \"system\",\n",
    "    \"\"\"\n",
    "    You are a professional doctor. Based on the following context, provide a detailed analysis and comprehensive advice regarding the patient's condition. Only include the sections \"종합 분석\" and \"종합적인 조언\" in your response. Do not include headings or introductory/concluding sentences. Answer questions using only the provided context. If you do not know the answer, simply state that you do not know; do not speculate or make up information.:\\n\\n{context}\"\n",
    "     \"\"\",\n",
    "     ),\n",
    "    (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:11.971096Z",
     "start_time": "2024-08-09T08:22:11.957061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def csv_files_to_json(file_paths, encoding='cp949'):\n",
    "    # Initialize a default dictionary to store dictionaries for each ID\n",
    "    json_data = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Iterate through each CSV file\n",
    "    for file_path in file_paths:\n",
    "        # Extract the file name without extension to use as a key\n",
    "        file_key = file_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        # Read the CSV file into a DataFrame with cp949 encoding\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        \n",
    "        # Group the data by 'id'\n",
    "        for id, group in df.groupby('id'):\n",
    "            # Convert the group data to a dictionary without the 'id' column\n",
    "            record = group.drop(columns=['id']).to_dict(orient='records')\n",
    "            # Append the record under the corresponding file name for the given id\n",
    "            json_data[str(id)][file_key].extend(record)\n",
    "    \n",
    "    # Convert the default dictionary to a regular dictionary and then to a JSON string\n",
    "    json_str = json.dumps(dict(json_data), indent=4)\n",
    "    \n",
    "    return json_str"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:11.986357Z",
     "start_time": "2024-08-09T08:22:11.972097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inbody_analysis(embedding_function, llm, prompt, id, question):\n",
    "    \n",
    "    # file_paths = ['./data/inbody.csv', './data/patients.csv','./data/reports.csv','./data/vital.csv']\n",
    "    file_paths = ['./data/col_value_change/inbody.csv']\n",
    "    \n",
    "    inbody_json = csv_files_to_json(file_paths)\n",
    "    json_data = json.loads(inbody_json)\n",
    "    \n",
    "    # 특정 id의 데이터 불러오기 (예: id가 '1'인 경우)\n",
    "    target_id = id\n",
    "    \n",
    "    # 해당 id가 존재하는지 확인\n",
    "    if target_id in json_data:\n",
    "        id_data = json_data[target_id]\n",
    "        print(f\"{target_id}에 대한 인바디 정보 분석 중...\")\n",
    "        json_output = json.dumps(id_data)\n",
    "    else:\n",
    "        print(f\"ID {target_id} not found.\")\n",
    "    \n",
    "    json_data = json.loads(json_output)\n",
    "    \n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "        \n",
    "    # The splitter can also output documents\n",
    "    docs = splitter.create_documents(texts=[json_data])\n",
    "    \n",
    "    # or a list of strings\n",
    "    # texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "    #벡터스토어 생성\n",
    "    inbody_vectorstore = FAISS.from_documents(docs, embedding_function)\n",
    "    #retriever\n",
    "    inbody_retriever = inbody_vectorstore.as_retriever()\n",
    "#     inbody_query = f\"\"\"\n",
    "#     context에 있는 정보를 정리하여 간단하게 출력하라. 현재 날짜는 :{time.strftime('%Y.%m.%d')}이며 context의 pat_sex는 환자의 성별, pat_birth는 환자의 탄생연도를 뜻한다.\\n\\n\n",
    "#     \"\"\"\n",
    "#     \n",
    "#     inbody_query += \"{context}\"\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "# [\n",
    "#     (\n",
    "#     \"system\",\n",
    "#     inbody_query,\n",
    "#      ),\n",
    "#     (\"human\", \"{question}\")\n",
    "#     ]\n",
    "# )\n",
    "    \n",
    "    inbody_chain = (\n",
    "        {\"context\": inbody_retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    inbody_response = inbody_chain.invoke(question)\n",
    "    return inbody_response"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:12.001887Z",
     "start_time": "2024-08-09T08:22:11.987359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vital_analysis(embedding_function, llm, prompt, id, question):\n",
    "    \n",
    "    # file_paths = ['./data/inbody.csv', './data/patients.csv','./data/reports.csv','./data/vital.csv']\n",
    "    file_paths = ['./data/col_value_change/vital.csv']\n",
    "    \n",
    "    vital_json = csv_files_to_json(file_paths)\n",
    "    json_data = json.loads(vital_json)\n",
    "    \n",
    "    # 특정 id의 데이터 불러오기 (예: id가 '1'인 경우)\n",
    "    target_id = id\n",
    "    \n",
    "    # 해당 id가 존재하는지 확인\n",
    "    if target_id in json_data:\n",
    "        id_data = json_data[target_id]\n",
    "        print(f\"{target_id}에 대한 바이탈 정보 분석 중...\")\n",
    "        json_output = json.dumps(id_data)\n",
    "    else:\n",
    "        print(f\"ID {target_id} not found.\")\n",
    "    \n",
    "    json_data = json.loads(json_output)\n",
    "    \n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "    \n",
    "    \n",
    "    # The splitter can also output documents\n",
    "    docs = splitter.create_documents(texts=[json_data])\n",
    "    \n",
    "    # or a list of strings\n",
    "    texts = splitter.split_text(json_data=json_data)\n",
    "    #벡터스토어 생성\n",
    "    vital_vectorstore = FAISS.from_documents(docs, embedding_function)\n",
    "    #retriever\n",
    "    vital_retriever = vital_vectorstore.as_retriever()\n",
    "#     vital_query = f\"\"\"\n",
    "#     context에 있는 정보를 정리하여 간단하게 출력하라. 현재 날짜는 :{time.strftime('%Y.%m.%d')}이며 context의 pat_sex는 환자의 성별, pat_birth는 환자의 탄생연도를 뜻한다.\\n\\n\n",
    "#     \"\"\"\n",
    "#     \n",
    "#     vital_query += \"{context}\"\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "# [\n",
    "#     (\n",
    "#     \"system\",\n",
    "#     vital_query,\n",
    "#      ),\n",
    "#     (\"human\", \"{question}\")\n",
    "#     ]\n",
    "# )\n",
    "    \n",
    "    vital_chain = (\n",
    "        {\"context\": vital_retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    vital_response = vital_chain.invoke(question)\n",
    "    return vital_response"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:12.094053Z",
     "start_time": "2024-08-09T08:22:12.080048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def patients_analysis(embedding_function, llm, id, question):\n",
    "    \n",
    "    # file_paths = ['./data/inbody.csv', './data/patients.csv','./data/reports.csv','./data/vital.csv']\n",
    "    file_paths = ['./data/col_value_change/patients.csv']\n",
    "    \n",
    "    patients_json = csv_files_to_json(file_paths)\n",
    "    json_data = json.loads(patients_json)\n",
    "    \n",
    "    # 특정 id의 데이터 불러오기 (예: id가 '1'인 경우)\n",
    "    target_id = id\n",
    "    \n",
    "    # 해당 id가 존재하는지 확인\n",
    "    if target_id in json_data:\n",
    "        id_data = json_data[target_id]\n",
    "        print(f\"{target_id}에 대한 환자 정보 분석 중...\")\n",
    "        json_output = json.dumps(id_data)\n",
    "    else:\n",
    "        print(f\"ID {target_id} not found.\")\n",
    "    \n",
    "    json_data = json.loads(json_output)\n",
    "    \n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "    \n",
    "    \n",
    "    # The splitter can also output documents\n",
    "    docs = splitter.create_documents(texts=[json_data])\n",
    "    \n",
    "    # or a list of strings\n",
    "    texts = splitter.split_text(json_data=json_data)\n",
    "    #벡터스토어 생성\n",
    "    patients_vectorstore = FAISS.from_documents(docs, embedding_function)\n",
    "    #retriever\n",
    "    patients_retriever = patients_vectorstore.as_retriever()\n",
    "    \n",
    "    patients_query = f\"\"\"\n",
    "    context에 있는 정보를 정리하여 간단하게 출력하라. 현재 날짜는 :{time.strftime('%Y.%m.%d')}이며 context의 pat_sex는 환자의 성별, pat_birth는 환자의 탄생연도를 뜻한다.\\n\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    patients_query += \"{context}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\n",
    "    \"system\",\n",
    "    patients_query,\n",
    "     ),\n",
    "    (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "    \n",
    "    patients_chain = (\n",
    "        {\"context\": patients_retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    patients_response = patients_chain.invoke(question)\n",
    "    return patients_response"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:12.432241Z",
     "start_time": "2024-08-09T08:22:12.413241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reports_analysis(embedding_function, llm, prompt, id, question):\n",
    "    \n",
    "    # file_paths = ['./data/inbody.csv', './data/reports.csv','./data/reports.csv','./data/vital.csv']\n",
    "    file_paths = ['./data/col_value_change/reports.csv']\n",
    "    \n",
    "    reports_json = csv_files_to_json(file_paths)\n",
    "    json_data = json.loads(reports_json)\n",
    "    \n",
    "    # 특정 id의 데이터 불러오기 (예: id가 '1'인 경우)\n",
    "    target_id = id\n",
    "    \n",
    "    # 해당 id가 존재하는지 확인\n",
    "    if target_id in json_data:\n",
    "        id_data = json_data[target_id]\n",
    "        print(f\"{target_id}에 대한 설문 정보 분석 중...\")\n",
    "        json_output = json.dumps(id_data)\n",
    "    else:\n",
    "        print(f\"ID {target_id} not found.\")\n",
    "    \n",
    "    json_data = json.loads(json_output)\n",
    "    \n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "    \n",
    "    \n",
    "    # The splitter can also output documents\n",
    "    docs = splitter.create_documents(texts=[json_data])\n",
    "    \n",
    "    # or a list of strings\n",
    "    texts = splitter.split_text(json_data=json_data)\n",
    "    #벡터스토어 생성\n",
    "    reports_vectorstore = FAISS.from_documents(docs, embedding_function)\n",
    "    #retriever\n",
    "    reports_retriever = reports_vectorstore.as_retriever()\n",
    "    \n",
    "#     reports_query = f\"\"\"\n",
    "#     context에 있는 정보를 정리하여 간단하게 출력하라. 현재 날짜는 :{time.strftime('%Y.%m.%d')}이며 context의 pat_sex는 환자의 성별, pat_birth는 환자의 탄생연도를 뜻한다.\\n\\n\n",
    "#     \"\"\"\n",
    "#     \n",
    "#     reports_query += \"{context}\"\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "# [\n",
    "#     (\n",
    "#     \"system\",\n",
    "#     reports_query,\n",
    "#      ),\n",
    "#     (\"human\", \"{question}\")\n",
    "#     ]\n",
    "# )\n",
    "    \n",
    "    reports_chain = (\n",
    "        {\"context\": reports_retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    reports_response = reports_chain.invoke(question)\n",
    "    return reports_response"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:22:13.212657Z",
     "start_time": "2024-08-09T08:22:13.201096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def final_response(id, question):\n",
    "    inbody_response = inbody_analysis(embedding_function, llm, analysis_prompt, id, question)\n",
    "    vital_response = vital_analysis(embedding_function, llm, analysis_prompt, id, question)\n",
    "    patients_response = patients_analysis(embedding_function, llm, id, question)\n",
    "    reports_resonse = reports_analysis(embedding_function, llm, analysis_prompt, id, question)\n",
    "    total_analysis = f\"\"\"\n",
    "    환자 정보 :\n",
    "    {patients_response}\n",
    "    \n",
    "    inbody 분석 : \n",
    "    {inbody_response}\n",
    "    \n",
    "    vital 분석 : \n",
    "    {vital_response}\n",
    "    \n",
    "    설문조사 분석 :\n",
    "    {reports_resonse} \n",
    "    \"\"\"\n",
    "    \n",
    "    final_query = f\"\"\"\n",
    "    You are a professional doctor. Based on the following context, provide a detailed analysis and comprehensive advice regarding the patient's condition. Only include the sections \"종합 분석\" and \"종합적인 조언\" in your response. Do not include headings or introductory/concluding sentences. Answer questions using only the provided context. If you do not know the answer, simply state that you do not know; do not speculate or make up information.:\\n\\n{total_analysis}\"\"\"\n",
    "    \n",
    "    final_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         final_query         \n",
    "         ),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    final_chain = (\n",
    "        {\"question\": RunnablePassthrough()}\n",
    "        | final_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    response = final_chain.invoke(question)\n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:24:40.430670Z",
     "start_time": "2024-08-09T08:24:36.809105Z"
    }
   },
   "cell_type": "code",
   "source": "print(final_response('7','건강정보에 대한 종합적인 소견을 말해'))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7에 대한 인바디 정보 분석 중...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[99], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mfinal_response\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m7\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m건강정보에 대한 종합적인 소견을 말해\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[97], line 2\u001B[0m, in \u001B[0;36mfinal_response\u001B[1;34m(id, question)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfinal_response\u001B[39m(\u001B[38;5;28mid\u001B[39m, question):\n\u001B[1;32m----> 2\u001B[0m     inbody_response \u001B[38;5;241m=\u001B[39m \u001B[43minbody_analysis\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manalysis_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     vital_response \u001B[38;5;241m=\u001B[39m vital_analysis(embedding_function, llm, analysis_prompt, \u001B[38;5;28mid\u001B[39m, question)\n\u001B[0;32m      4\u001B[0m     patients_response \u001B[38;5;241m=\u001B[39m patients_analysis(embedding_function, llm, \u001B[38;5;28mid\u001B[39m, question)\n",
      "Cell \u001B[1;32mIn[93], line 31\u001B[0m, in \u001B[0;36minbody_analysis\u001B[1;34m(embedding_function, llm, prompt, id, question)\u001B[0m\n\u001B[0;32m     25\u001B[0m docs \u001B[38;5;241m=\u001B[39m splitter\u001B[38;5;241m.\u001B[39mcreate_documents(texts\u001B[38;5;241m=\u001B[39m[json_data])\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# or a list of strings\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# texts = splitter.split_text(json_data=json_data)\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#벡터스토어 생성\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m inbody_vectorstore \u001B[38;5;241m=\u001B[39m \u001B[43mFAISS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#retriever\u001B[39;00m\n\u001B[0;32m     33\u001B[0m inbody_retriever \u001B[38;5;241m=\u001B[39m inbody_vectorstore\u001B[38;5;241m.\u001B[39mas_retriever()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1058\u001B[0m, in \u001B[0;36mVectorStore.from_documents\u001B[1;34m(cls, documents, embedding, **kwargs)\u001B[0m\n\u001B[0;32m   1056\u001B[0m texts \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m   1057\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m-> 1058\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_texts(texts, embedding, metadatas\u001B[38;5;241m=\u001B[39mmetadatas, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:930\u001B[0m, in \u001B[0;36mFAISS.from_texts\u001B[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[0m\n\u001B[0;32m    903\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_texts\u001B[39m(\n\u001B[0;32m    905\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    911\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FAISS:\n\u001B[0;32m    912\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[0;32m    913\u001B[0m \n\u001B[0;32m    914\u001B[0m \u001B[38;5;124;03m    This is a user friendly interface that:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[0;32m    929\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 930\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    931\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__from(\n\u001B[0;32m    932\u001B[0m         texts,\n\u001B[0;32m    933\u001B[0m         embeddings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    938\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:558\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[1;34m(self, texts, chunk_size)\u001B[0m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[0;32m    557\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[1;32m--> 558\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:456\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[1;34m(self, texts, engine, chunk_size)\u001B[0m\n\u001B[0;32m    454\u001B[0m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[1;32m--> 456\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m    457\u001B[0m         \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtokens[i : i \u001B[38;5;241m+\u001B[39m _chunk_size], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invocation_params\n\u001B[0;32m    458\u001B[0m     )\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    460\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\resources\\embeddings.py:114\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    108\u001B[0m         embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[0;32m    109\u001B[0m             base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    110\u001B[0m         )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1258\u001B[0m     )\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:936\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    928\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    929\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    934\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    935\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 936\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1025\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1024\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1026\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[43m        \u001B[49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1030\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1032\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1074\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1072\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1074\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1025\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1024\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1026\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[43m        \u001B[49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1030\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1032\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1074\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1072\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1074\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lang_Chain\\lib\\site-packages\\openai\\_base_client.py:1040\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1037\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1039\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1040\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1043\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1044\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1048\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[0;32m   1049\u001B[0m )\n",
      "\u001B[1;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:20:47.732980Z",
     "start_time": "2024-08-09T08:20:47.732980Z"
    }
   },
   "cell_type": "code",
   "source": "print(final_response('1','추천하는 운동을 알려줘'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(final_response('1','추천하는 식단들을 설명해'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
